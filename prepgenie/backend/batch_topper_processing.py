"""
Batch Topper Processing Script
Processes topper PDFs in batches to avoid overwhelming the system
"""

import os
import sys
import asyncio
import logging
from datetime import datetime

# Add the project root to Python path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from topper_pdf_processing_pipeline import TopperPDFProcessor

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'batch_processing_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

async def process_in_batches(batch_size: int = 5):
    """Process topper PDFs in batches"""
    
    logger.info(f"üöÄ Starting Batch Processing (batch size: {batch_size})")
    logger.info("=" * 60)
    
    processor = TopperPDFProcessor()
    
    if not await processor.initialize():
        logger.error("‚ùå Failed to initialize - stopping")
        return
    
    # Discover all PDFs
    pdf_files = await processor.discover_topper_pdfs()
    
    if not pdf_files:
        logger.error("‚ùå No PDF files found")
        return
    
    logger.info(f"üìö Found {len(pdf_files)} PDFs to process in {(len(pdf_files) + batch_size - 1) // batch_size} batches")
    
    # Process in batches
    total_processed = 0
    total_failed = 0
    
    for batch_num in range(0, len(pdf_files), batch_size):
        batch_files = pdf_files[batch_num:batch_num + batch_size]
        batch_index = (batch_num // batch_size) + 1
        
        logger.info(f"\nüì¶ Processing Batch {batch_index}: {len(batch_files)} files")
        logger.info("-" * 40)
        
        batch_processed = 0
        batch_failed = 0
        
        for pdf_file in batch_files:
            logger.info(f"üîÑ Processing: {os.path.basename(pdf_file)}")
            
            success = await processor.process_single_pdf(pdf_file)
            
            if success:
                batch_processed += 1
                total_processed += 1
                logger.info(f"‚úÖ Success - {os.path.basename(pdf_file)}")
            else:
                batch_failed += 1
                total_failed += 1
                logger.error(f"‚ùå Failed - {os.path.basename(pdf_file)}")
        
        logger.info(f"\nüìä Batch {batch_index} Summary:")
        logger.info(f"   ‚úÖ Successful: {batch_processed}")
        logger.info(f"   ‚ùå Failed: {batch_failed}")
        logger.info(f"   üìà Total so far: {total_processed} successful, {total_failed} failed")
        
        # Brief pause between batches
        if batch_num + batch_size < len(pdf_files):
            logger.info("‚è∏Ô∏è Brief pause before next batch...")
            await asyncio.sleep(2)
    
    # Final summary
    logger.info("\n" + "=" * 60)
    logger.info("üèÅ BATCH PROCESSING COMPLETE")
    logger.info("=" * 60)
    logger.info(f"‚úÖ Total successful: {total_processed}")
    logger.info(f"‚ùå Total failed: {total_failed}")
    logger.info(f"üìä Success rate: {(total_processed / len(pdf_files) * 100):.1f}%")
    
    # Vector database final stats
    try:
        stats = await processor.vector_service.get_collection_stats()
        logger.info(f"üî¢ Final Vector DB Stats: {stats}")
    except Exception as e:
        logger.error(f"Could not get final vector stats: {e}")
    
    await processor.cleanup()
    logger.info("üßπ Processing complete!")

async def quick_process_top_n(n: int = 3):
    """Process just the first N PDFs for quick testing"""
    
    logger.info(f"üß™ Quick Processing - Top {n} PDFs")
    logger.info("-" * 40)
    
    processor = TopperPDFProcessor()
    
    if not await processor.initialize():
        logger.error("‚ùå Failed to initialize")
        return
    
    pdf_files = await processor.discover_topper_pdfs()
    
    if not pdf_files:
        logger.error("‚ùå No PDF files found")
        return
    
    # Process just the first N files
    test_files = pdf_files[:n]
    logger.info(f"Processing {len(test_files)} files for quick test")
    
    for i, pdf_file in enumerate(test_files, 1):
        logger.info(f"\nüîÑ Processing {i}/{len(test_files)}: {os.path.basename(pdf_file)}")
        success = await processor.process_single_pdf(pdf_file)
        
        if success:
            logger.info(f"‚úÖ Success!")
        else:
            logger.error(f"‚ùå Failed!")
    
    # Quick vector search test
    try:
        logger.info("\nüîç Testing vector search...")
        results = await processor.vector_service.search_similar_topper_answers(
            query_question="Discuss digital governance initiatives in India",
            student_answer="Digital India has transformed governance through technology",
            filters={'exam_year': 2024},
            limit=5
        )
        logger.info(f"‚úÖ Search test: {len(results)} results found")
    except Exception as e:
        logger.error(f"‚ùå Search test failed: {e}")
    
    await processor.cleanup()

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Batch Topper PDF Processing")
    parser.add_argument("--mode", choices=["quick", "batch", "all"], default="quick",
                       help="Processing mode: quick (3 PDFs), batch (batched), all (all at once)")
    parser.add_argument("--batch_size", type=int, default=5,
                       help="Batch size for batch processing")
    parser.add_argument("--quick_count", type=int, default=3,
                       help="Number of PDFs for quick processing")
    
    args = parser.parse_args()
    
    if args.mode == "quick":
        print(f"üß™ Quick Processing Mode - Processing {args.quick_count} PDFs")
        asyncio.run(quick_process_top_n(args.quick_count))
    elif args.mode == "batch":
        print(f"üì¶ Batch Processing Mode - Batch size: {args.batch_size}")
        asyncio.run(process_in_batches(args.batch_size))
    elif args.mode == "all":
        print("üöÄ Full Processing Mode - All PDFs at once")
        from topper_pdf_processing_pipeline import main
        asyncio.run(main())
    
    print("\n‚úÖ Processing complete!")
